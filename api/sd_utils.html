<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>ccai9012.sd_utils API documentation</title>
<meta name="description" content="CCAI9012 Toolkit">
<link rel="stylesheet" href="../docs-style.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>
/* Additional styles for API documentation content */
.api-content {
    max-width: none;
}
.api-content code {
    background-color: var(--code-bg);
    padding: 0.2rem 0.4rem;
    border-radius: 3px;
    font-family: 'Courier New', monospace;
    font-size: 0.875rem;
    color: #e11d48;
}
.api-content pre code {
    background: var(--code-bg);
    padding: 1rem;
    display: block;
    color: var(--text-color);
}
.api-content .name {
    background: #eee;
    font-size: 0.85em;
    padding: 5px 10px;
    display: inline-block;
    border-radius: 4px;
}
.api-content .name:hover {
    background: #e0e0e0;
}
.api-content dl {
    margin-bottom: 2em;
}
.api-content dd {
    margin: 0 0 1em 2em;
}
.api-content .desc {
    margin-top: 1em;
}
.api-content h2 {
    margin-top: 2em;
}
.api-content .ident {
    color: #900;
    font-weight: bold;
}
</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"' + '"' + '"', "'" + "'" + "'"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"' + '"' + '"</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
    <div class="container">
        <nav id="sidebar">
            <div class="sidebar-header">
                <h2>CCAI9012</h2>
            </div>
            <ul class="nav-menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../installation.html">Installation Guide</a></li>
                <li><a href="../starter_kits.html">Starter Kits</a></li>
                <li><a href="../reading_material.html">Reading Materials</a></li>
                <li><a href="../datasets.html">Datasets Reference</a></li>
                <li><a href="index.html">API Documentation</a></li>
                <li style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--sidebar-hover);">
                    <span style="color: #94a3b8; font-size: 0.875rem; padding-left: 1.5rem; display: block; margin-bottom: 0.5rem;">API Modules</span>
                </li>
                <li style="padding-left: 1rem;"><a href="gan_utils.html">gan_utils</a></li>
                <li style="padding-left: 1rem;"><a href="llm_utils.html">llm_utils</a></li>
                <li style="padding-left: 1rem;"><a href="multi_modal_utils.html">multi_modal_utils</a></li>
                <li style="padding-left: 1rem;"><a href="nn_utils.html">nn_utils</a></li>
                <li style="padding-left: 1rem;"><a href="sd_utils.html">sd_utils</a></li>
                <li style="padding-left: 1rem;"><a href="svi_utils.html">svi_utils</a></li>
                <li style="padding-left: 1rem;"><a href="viz_utils.html">viz_utils</a></li>
                <li style="padding-left: 1rem;"><a href="yolo_utils.html">yolo_utils</a></li>
            </ul>
        </nav>

        <main id="content" class="api-content">

<article id="content">
<header>
<h1 class="title">Module <code>ccai9012.sd_utils</code></h1>
</header>
<section id="section-intro">
<h1 id="stable-diffusion-utilities-module">Stable Diffusion Utilities Module</h1>
<p>This module provides utilities for text-to-image generation using Stable Diffusion models.
It offers a flexible interface to generate images either through the Hugging Face Inference API
(cloud-based) or locally using downloaded models.</p>
<p>The module is designed to simplify the process of generating images from text prompts,
handling authentication, model loading, and providing consistent interfaces regardless
of the execution mode chosen.</p>
<p>Main components:
- API key management: Secure handling of Hugging Face API keys
- SDClient: A versatile client that can operate in two modes:
- "inference": Uses Hugging Face's Inference API (cloud-based, faster startup)
- "local": Loads and runs models locally (higher throughput for multiple generations)</p>
<h2 id="usage">Usage</h2>
<p>client = SDClient(mode="inference")
# Use Hugging Face Inference API
images = client.generate_images("a photo of a cat", num_images=2)</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ccai9012.sd_utils.get_hf_api_key"><code class="name flex">
<span>def <span class="ident">get_hf_api_key</span></span>(<span>env_var: str = 'HUGGINGFACE_API_KEY') ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hf_api_key(env_var: str = &#34;HUGGINGFACE_API_KEY&#34;) -&gt; str:
    &#34;&#34;&#34;
    Get Hugging Face API key from environment variables or prompt user securely.

    This function first attempts to retrieve the API key from the specified environment variable.
    If the key is not found, it prompts the user to enter it securely (without displaying it),
    then stores it in the environment variable for future use within the session.

    Args:
        env_var (str): The name of the environment variable to check for the API key.
                      Defaults to &#34;HUGGINGFACE_API_KEY&#34;.

    Returns:
        str: The Hugging Face API key.
    &#34;&#34;&#34;
    api_key = os.getenv(env_var)
    if not api_key:
        api_key = getpass.getpass(f&#34;Enter your {env_var}: &#34;)
        os.environ[env_var] = api_key
    return api_key</code></pre>
</details>
<div class="desc"><p>Get Hugging Face API key from environment variables or prompt user securely.</p>
<p>This function first attempts to retrieve the API key from the specified environment variable.
If the key is not found, it prompts the user to enter it securely (without displaying it),
then stores it in the environment variable for future use within the session.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env_var</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the environment variable to check for the API key.
Defaults to "HUGGINGFACE_API_KEY".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The Hugging Face API key.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ccai9012.sd_utils.SDClient"><code class="flex name class">
<span>class <span class="ident">SDClient</span></span>
<span>(</span><span>mode: str = 'inference',<br>model_id: str = 'stabilityai/stable-diffusion-2-base',<br>cache_dir: str = None,<br>use_auth_token: str = None,<br>device: str = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SDClient:
    &#34;&#34;&#34;
    Stable Diffusion Client for text-to-image generation.

    This class provides a unified interface for generating images from text prompts,
    supporting two operational modes:

    1. &#34;inference&#34; mode: Uses Hugging Face&#39;s Inference API (cloud-based)
       - Advantages: Faster startup, no model downloads, lower memory requirements
       - Use case: Quick testing, limited hardware resources

    2. &#34;local&#34; mode: Loads and runs models locally via diffusers library
       - Advantages: Higher throughput for multiple generations, more control
       - Use case: Batch processing, offline usage, custom pipelines

    The class handles model loading, device management, and API authentication
    automatically, providing a simple interface for generating images.

    Attributes:
        mode (str): Operating mode (&#34;inference&#34; or &#34;local&#34;).
        model_id (str): Hugging Face model repository ID.
        device (str): Torch device for computation (&#34;cuda&#34; or &#34;cpu&#34;).
        api_key (str): Hugging Face API key for authentication.
        client: Inference client (in &#34;inference&#34; mode).
        pipe: StableDiffusionPipeline instance (in &#34;local&#34; mode).
        cache_dir (str): Directory to cache downloaded models (in &#34;local&#34; mode).
    &#34;&#34;&#34;

    def __init__(
        self,
        mode: str = &#34;inference&#34;,  # &#34;inference&#34; or &#34;local&#34;
        model_id: str = &#34;stabilityai/stable-diffusion-2-base&#34;,
        cache_dir: str = None,
        use_auth_token: str = None,
        device: str = None,
    ):
        &#34;&#34;&#34;
        Initialize the Stable Diffusion Client with specified configuration.

        Sets up the client based on the chosen operational mode, handling authentication,
        model loading, and device selection automatically.

        Args:
            mode (str): Operational mode for image generation. Options:
                       - &#34;inference&#34;: Use Hugging Face&#39;s cloud-based Inference API.
                       - &#34;local&#34;: Load and run the model locally using diffusers library.
                       Defaults to &#34;inference&#34;.
            model_id (str): Hugging Face model repository ID for the Stable Diffusion model.
                          Defaults to &#34;stabilityai/stable-diffusion-2-base&#34;.
            cache_dir (str, optional): Local directory to cache downloaded models in &#34;local&#34; mode.
                                     If None, defaults to &#34;./models&#34; as an absolute path.
            use_auth_token (str, optional): Hugging Face API token for authentication.
                                          If None, will attempt to retrieve or prompt for it.
            device (str, optional): PyTorch device to use for computation.
                                  If None, automatically selects CUDA if available, otherwise CPU.

        Raises:
            ValueError: If the specified mode is not &#34;inference&#34; or &#34;local&#34;.
        &#34;&#34;&#34;
        self.mode = mode.lower()
        self.model_id = model_id
        self.device = device or (&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
        self.api_key = use_auth_token or get_hf_api_key()

        if self.mode == &#34;inference&#34;:
            self.client = InferenceClient(
                provider=&#34;hf-inference&#34;,
                headers={&#34;X-Use-Cache&#34;: &#34;false&#34;},
                api_key=self.api_key,
            )
        elif self.mode == &#34;local&#34;:
            if cache_dir is None:
                cache_dir = os.path.abspath(&#34;./models&#34;)
            self.cache_dir = cache_dir
            self.pipe = StableDiffusionPipeline.from_pretrained(
                self.model_id,
                use_auth_token=self.api_key,
                cache_dir=self.cache_dir,
                torch_dtype=torch.float16 if self.device.startswith(&#34;cuda&#34;) else torch.float32,
            ).to(self.device)
        else:
            raise ValueError(&#34;mode must be &#39;inference&#39; or &#39;local&#39;&#34;)

    def generate_images(
        self,
        prompt: str,
        num_images: int = 1,
        guidance_scale: float = 7.5,
        num_inference_steps: int = 50,
        seed: int = None,
        display_images: bool = True,
    ):
        &#34;&#34;&#34;
        Generate images from a text prompt using Stable Diffusion.

        This method generates one or more images based on the provided text prompt,
        using either the Hugging Face Inference API (in &#34;inference&#34; mode) or a locally
        loaded model (in &#34;local&#34; mode). Generated images can optionally be displayed
        inline in Jupyter notebooks.

        Args:
            prompt (str): Text description of the image to generate.
            num_images (int, optional): Number of images to generate. Defaults to 1.
            guidance_scale (float, optional): Classifier-free guidance scale, controlling how
                                           closely the image follows the prompt. Higher values
                                           give more prompt adherence but less diversity.
                                           Defaults to 7.5.
            num_inference_steps (int, optional): Number of denoising steps in the diffusion process.
                                              More steps typically yield higher quality images
                                              but take longer to generate. Defaults to 50.
            seed (int, optional): Random seed for reproducible generation.
                                If None, a random seed is used. Defaults to None.
            display_images (bool, optional): Whether to display generated images inline
                                          in Jupyter notebook environments. Defaults to True.

        Returns:
            list: A list of PIL.Image objects representing the generated images.
        &#34;&#34;&#34;
        images = []

        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None

        if self.mode == &#34;inference&#34;:
            for _ in range(num_images):
                img = self.client.text_to_image(
                    prompt=prompt,
                    model=self.model_id,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps,
                    seed=seed,
                )
                images.append(img)
        else:  # local mode
            for _ in range(num_images):
                img = self.pipe(
                    prompt,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps,
                    generator=generator,
                ).images[0]
                images.append(img)

        if display_images:
            for img in images:
                display(img)

        return images</code></pre>
</details>
<div class="desc"><p>Stable Diffusion Client for text-to-image generation.</p>
<p>This class provides a unified interface for generating images from text prompts,
supporting two operational modes:</p>
<ol>
<li>"inference" mode: Uses Hugging Face's Inference API (cloud-based)</li>
<li>Advantages: Faster startup, no model downloads, lower memory requirements</li>
<li>
<p>Use case: Quick testing, limited hardware resources</p>
</li>
<li>
<p>"local" mode: Loads and runs models locally via diffusers library</p>
</li>
<li>Advantages: Higher throughput for multiple generations, more control</li>
<li>Use case: Batch processing, offline usage, custom pipelines</li>
</ol>
<p>The class handles model loading, device management, and API authentication
automatically, providing a simple interface for generating images.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>Operating mode ("inference" or "local").</dd>
<dt><strong><code>model_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Hugging Face model repository ID.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>Torch device for computation ("cuda" or "cpu").</dd>
<dt><strong><code>api_key</code></strong> :&ensp;<code>str</code></dt>
<dd>Hugging Face API key for authentication.</dd>
<dt><strong><code>client</code></strong></dt>
<dd>Inference client (in "inference" mode).</dd>
<dt><strong><code>pipe</code></strong></dt>
<dd>StableDiffusionPipeline instance (in "local" mode).</dd>
<dt><strong><code>cache_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Directory to cache downloaded models (in "local" mode).</dd>
</dl>
<p>Initialize the Stable Diffusion Client with specified configuration.</p>
<p>Sets up the client based on the chosen operational mode, handling authentication,
model loading, and device selection automatically.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>Operational mode for image generation. Options:
- "inference": Use Hugging Face's cloud-based Inference API.
- "local": Load and run the model locally using diffusers library.
Defaults to "inference".</dd>
<dt><strong><code>model_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Hugging Face model repository ID for the Stable Diffusion model.
Defaults to "stabilityai/stable-diffusion-2-base".</dd>
<dt><strong><code>cache_dir</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Local directory to cache downloaded models in "local" mode.
If None, defaults to "./models" as an absolute path.</dd>
<dt><strong><code>use_auth_token</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Hugging Face API token for authentication.
If None, will attempt to retrieve or prompt for it.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>PyTorch device to use for computation.
If None, automatically selects CUDA if available, otherwise CPU.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the specified mode is not "inference" or "local".</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="ccai9012.sd_utils.SDClient.generate_images"><code class="name flex">
<span>def <span class="ident">generate_images</span></span>(<span>self,<br>prompt: str,<br>num_images: int = 1,<br>guidance_scale: float = 7.5,<br>num_inference_steps: int = 50,<br>seed: int = None,<br>display_images: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_images(
    self,
    prompt: str,
    num_images: int = 1,
    guidance_scale: float = 7.5,
    num_inference_steps: int = 50,
    seed: int = None,
    display_images: bool = True,
):
    &#34;&#34;&#34;
    Generate images from a text prompt using Stable Diffusion.

    This method generates one or more images based on the provided text prompt,
    using either the Hugging Face Inference API (in &#34;inference&#34; mode) or a locally
    loaded model (in &#34;local&#34; mode). Generated images can optionally be displayed
    inline in Jupyter notebooks.

    Args:
        prompt (str): Text description of the image to generate.
        num_images (int, optional): Number of images to generate. Defaults to 1.
        guidance_scale (float, optional): Classifier-free guidance scale, controlling how
                                       closely the image follows the prompt. Higher values
                                       give more prompt adherence but less diversity.
                                       Defaults to 7.5.
        num_inference_steps (int, optional): Number of denoising steps in the diffusion process.
                                          More steps typically yield higher quality images
                                          but take longer to generate. Defaults to 50.
        seed (int, optional): Random seed for reproducible generation.
                            If None, a random seed is used. Defaults to None.
        display_images (bool, optional): Whether to display generated images inline
                                      in Jupyter notebook environments. Defaults to True.

    Returns:
        list: A list of PIL.Image objects representing the generated images.
    &#34;&#34;&#34;
    images = []

    if seed is not None:
        generator = torch.Generator(device=self.device).manual_seed(seed)
    else:
        generator = None

    if self.mode == &#34;inference&#34;:
        for _ in range(num_images):
            img = self.client.text_to_image(
                prompt=prompt,
                model=self.model_id,
                guidance_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                seed=seed,
            )
            images.append(img)
    else:  # local mode
        for _ in range(num_images):
            img = self.pipe(
                prompt,
                guidance_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                generator=generator,
            ).images[0]
            images.append(img)

    if display_images:
        for img in images:
            display(img)

    return images</code></pre>
</details>
<div class="desc"><p>Generate images from a text prompt using Stable Diffusion.</p>
<p>This method generates one or more images based on the provided text prompt,
using either the Hugging Face Inference API (in "inference" mode) or a locally
loaded model (in "local" mode). Generated images can optionally be displayed
inline in Jupyter notebooks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prompt</code></strong> :&ensp;<code>str</code></dt>
<dd>Text description of the image to generate.</dd>
<dt><strong><code>num_images</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of images to generate. Defaults to 1.</dd>
<dt><strong><code>guidance_scale</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Classifier-free guidance scale, controlling how
closely the image follows the prompt. Higher values
give more prompt adherence but less diversity.
Defaults to 7.5.</dd>
<dt><strong><code>num_inference_steps</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of denoising steps in the diffusion process.
More steps typically yield higher quality images
but take longer to generate. Defaults to 50.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Random seed for reproducible generation.
If None, a random seed is used. Defaults to None.</dd>
<dt><strong><code>display_images</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to display generated images inline
in Jupyter notebook environments. Defaults to True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A list of PIL.Image objects representing the generated images.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul>
<li><a href="#stable-diffusion-utilities-module">Stable Diffusion Utilities Module</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ccai9012" href="index.html">ccai9012</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ccai9012.sd_utils.get_hf_api_key" href="#ccai9012.sd_utils.get_hf_api_key">get_hf_api_key</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ccai9012.sd_utils.SDClient" href="#ccai9012.sd_utils.SDClient">SDClient</a></code></h4>
<ul class="">
<li><code><a title="ccai9012.sd_utils.SDClient.generate_images" href="#ccai9012.sd_utils.SDClient.generate_images">generate_images</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>

        </main>
    </div>
</body>
</html>